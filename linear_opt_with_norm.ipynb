{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec42dd55",
   "metadata": {},
   "source": [
    "# Linear Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7450e12",
   "metadata": {},
   "source": [
    "Optimizes a linear straight-line path from a point on the viable brain surface region to a point on the subthalamic nucleus (STN) for DBS electrode planning.\n",
    "\n",
    "Treats the following as obstacles:\n",
    "- Sulci\n",
    "- Ventricles\n",
    "- Corpus Callosum (prevent brain crossing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1ca02f",
   "metadata": {},
   "source": [
    "**Objective Functions**\n",
    "- Distance to obstacles\n",
    "- Minimum path distance  *(not optimized yet)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5253184e",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a0b5ab",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a94cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import vedo\n",
    "from vedo import Volume, show, Line, merge, Plotter\n",
    "\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b427aa6a",
   "metadata": {},
   "source": [
    "### Configuration and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf206b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define new base directory and sub-folder structure\n",
    "base_dir = \"./FINAL_BRAIN_ATLAS/final_nii_files\"\n",
    "\n",
    "# Entry zones (viable surface regions for path start)\n",
    "entry_zone_dir = os.path.join(base_dir, \"entry_zone\")\n",
    "left_entry_zone_dir = os.path.join(entry_zone_dir, \"LEFT_ENTRY_ZONE\")\n",
    "right_entry_zone_dir = os.path.join(entry_zone_dir, \"RIGHT_ENTRY_ZONE\")\n",
    "\n",
    "# Subthalamic nucleus (targets)\n",
    "stn_root_dir = os.path.join(base_dir, \"subthalamic_nucleus\")\n",
    "left_stn_dir = os.path.join(stn_root_dir, \"LEFT_STN\")\n",
    "right_stn_dir = os.path.join(stn_root_dir, \"RIGHT_STN\")\n",
    "\n",
    "# Full brain (for context / outline only)\n",
    "full_brain_dir = os.path.join(base_dir, \"full_brain\")\n",
    "\n",
    "# Obstacles (same role as before)\n",
    "obstacles_dir = os.path.join(base_dir, \"obstacles\")\n",
    "\n",
    "print(\"Base directory:\", base_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76da8d2c",
   "metadata": {},
   "source": [
    "### Load Brain Structures\n",
    "\n",
    "Recursively load all `.nii` / `.nii.gz` files under each designated root:\n",
    "- Entry Zones (LEFT/RIGHT) viable start regions (light brown, low alpha)\n",
    "- STN Targets (LEFT/RIGHT) (distinct greens)\n",
    "- Obstacles (red)\n",
    "- Full Brain outline (very light grey, minimal alpha)\n",
    "\n",
    "All subfolder contents are merged later per category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16468e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists for different structure types\n",
    "left_entry_zone_structures = []\n",
    "right_entry_zone_structures = []\n",
    "left_stn_structures = []\n",
    "right_stn_structures = []\n",
    "allowed_left_entry_zone_structures = []\n",
    "allowed_right_entry_zone_structures = []\n",
    "obstacle_structures = []\n",
    "full_brain_structures = []\n",
    "\n",
    "# Loader utility: walk all subfolders and load every .nii / .nii.gz\n",
    "def load_folder_surfaces(root_folder, color, alpha=1.0, outer_surf=False):\n",
    "    loaded = []\n",
    "    if not os.path.isdir(root_folder):\n",
    "        print(f\"[WARN] Folder not found: {root_folder}\")\n",
    "        return loaded\n",
    "    \n",
    "    if outer_surf:\n",
    "        #do something\n",
    "        for dirpath, dirnames, filenames in os.walk(root_folder):\n",
    "            nii_files = [f for f in filenames if f.lower().endswith('.nii') or f.lower().endswith('.nii.gz')]\n",
    "            for fname in nii_files:\n",
    "                fpath = os.path.join(dirpath, fname)\n",
    "                rel = os.path.relpath(fpath, root_folder)\n",
    "                print(f\"  Loading structure: {rel}\")\n",
    "                vol = Volume(fpath)\n",
    "                surf = vol.isosurface()\n",
    "                centre = surf.points.mean(axis=0)\n",
    "                components = surf.split()\n",
    "                outer_surfaces = []\n",
    "\n",
    "                for comp in components:\n",
    "                    comp.compute_normals(points=False, cells=True, consistency=True)\n",
    "                    verts = np.array(comp.points)\n",
    "                    faces = np.array(comp.cells)\n",
    "                    norms = np.array(comp.celldata[\"Normals\"])\n",
    "\n",
    "                    faces_centre = verts[faces].mean(axis=1)\n",
    "                    vectors = faces_centre - centre\n",
    "                    dots = np.einsum(\"ij,ij->i\", norms, vectors)\n",
    "                    keep = dots > 0 \n",
    "\n",
    "                    if keep.sum() == 0:\n",
    "                        continue\n",
    "\n",
    "                    outer_faces = faces[keep]\n",
    "                    outer = vedo.Mesh([verts, outer_faces]) # Hadd to from vedo list\n",
    "                    outer_surfaces.append(outer)\n",
    "                outer_surfaces = merge(outer_surfaces)\n",
    "                outer_surfaces.c(color).alpha(alpha)\n",
    "                loaded.append(outer_surfaces)\n",
    "    else:\n",
    "        for dirpath, dirnames, filenames in os.walk(root_folder):\n",
    "            nii_files = [f for f in filenames if f.lower().endswith('.nii') or f.lower().endswith('.nii.gz')]\n",
    "            for fname in nii_files:\n",
    "                fpath = os.path.join(dirpath, fname)\n",
    "                rel = os.path.relpath(fpath, root_folder)\n",
    "                print(f\"  Loading structure: {rel}\")\n",
    "                vol = Volume(fpath)\n",
    "                surf = vol.isosurface()\n",
    "                surf.c(color).alpha(alpha)\n",
    "                loaded.append(surf)\n",
    "    if not loaded:\n",
    "        print(f\"  [INFO] No NIfTI files found under {root_folder}\")\n",
    "    return loaded\n",
    "\n",
    "print(\"\\nLoading Entry Zones (LEFT)...\")\n",
    "left_entry_zone_structures = load_folder_surfaces(left_entry_zone_dir, color=(0.95, 0.65, 0.85), alpha=0.04)\n",
    "print(\"Loading Entry Zones (RIGHT)...\")\n",
    "right_entry_zone_structures = load_folder_surfaces(right_entry_zone_dir, color=(0.95, 0.75, 0.85), alpha=0.04)\n",
    "\n",
    "print(\"\\nLoading Entry Zones (LEFT)...\")\n",
    "allowed_left_entry_zone_structures = load_folder_surfaces(left_entry_zone_dir, color=(0.95, 0.65, 0.85), alpha=0.04, outer_surf=True)\n",
    "print(\"Loading Entry Zones (RIGHT)...\")\n",
    "allowed_right_entry_zone_structures = load_folder_surfaces(right_entry_zone_dir, color=(0.95, 0.75, 0.85), alpha=0.04, outer_surf=True)\n",
    "\n",
    "# STN folders (may already be deepest; recursion still safe)\n",
    "print(\"\\nLoading STN (LEFT)...\")\n",
    "left_stn_structures = load_folder_surfaces(left_stn_dir, color=\"green\", alpha=1.0)\n",
    "print(\"Loading STN (RIGHT)...\")\n",
    "right_stn_structures = load_folder_surfaces(right_stn_dir, color=\"green\", alpha=1.0)\n",
    "\n",
    "print(\"\\nLoading Obstacles...\")\n",
    "obstacle_structures = load_folder_surfaces(obstacles_dir, color=\"red\", alpha=0.7)\n",
    "\n",
    "print(\"\\nLoading Full Brain Outline...\")\n",
    "full_brain_structures = load_folder_surfaces(full_brain_dir, color=\"grey\", alpha=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed3254c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot and show the outer entry zone\n",
    "actors1 = []\n",
    "actors1 += allowed_left_entry_zone_structures\n",
    "actors1 += allowed_right_entry_zone_structures\n",
    "show(actors1, axes=1, viewup=\"z\", title=\"Entry Zones w/ Normals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0355644f",
   "metadata": {},
   "outputs": [],
   "source": [
    "actors2 = []\n",
    "\n",
    "# Add the meshes\n",
    "actors2 += allowed_left_entry_zone_structures\n",
    "actors2 += allowed_right_entry_zone_structures\n",
    "\n",
    "# Add normals for all meshes\n",
    "for mesh in allowed_left_entry_zone_structures + allowed_right_entry_zone_structures:\n",
    "    centre = mesh.points.mean(axis=0)\n",
    "    mesh.compute_normals(points=False, cells=True, consistency=True)\n",
    "    verts = np.array(mesh.points)\n",
    "    faces = np.array(mesh.cells)\n",
    "    norms = np.array(mesh.celldata[\"Normals\"])\n",
    "\n",
    "    faces_centre = verts[faces].mean(axis=1)\n",
    "    vectors = faces_centre - centre\n",
    "    dots = np.einsum(\"ij,ij->i\", norms, vectors)\n",
    "    red = dots > 0 \n",
    "    blue = ~red\n",
    "\n",
    "    if red.any():\n",
    "        arrows_red = vedo.Arrows(faces_centre[red], faces_centre[red] + norms[red]*5, c=\"red\")\n",
    "        actors2.append(arrows_red)\n",
    "    if blue.any():\n",
    "        arrows_blue = vedo.Arrows(faces_centre[blue], faces_centre[blue] + norms[blue]*5, c=\"blue\")\n",
    "        actors2.append(arrows_blue)\n",
    "        print(\"Normal toward the centre is found\")\n",
    "\n",
    "# Show everything together\n",
    "show(actors2, axes=1, viewup=\"z\", title=\"Entry Zones with Normals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124b851d",
   "metadata": {},
   "source": [
    "### Merge Structures\n",
    "\n",
    "Combine individual structures into single meshes for:\n",
    "- LEFT/RIGHT Entry Zones (start sampling)\n",
    "- LEFT/RIGHT STN (target sampling hemisphere-consistent)\n",
    "- Obstacles (for intersection & distance)\n",
    "- Full Brain (context outline only)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f56843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge structures into single meshes\n",
    "merged_left_entry_zone = merge(left_entry_zone_structures) if left_entry_zone_structures else None\n",
    "if merged_left_entry_zone: merged_left_entry_zone.alpha(0.04)\n",
    "\n",
    "merged_right_entry_zone = merge(right_entry_zone_structures) if right_entry_zone_structures else None\n",
    "if merged_right_entry_zone: merged_right_entry_zone.alpha(0.04)\n",
    "\n",
    "merged_allowed_left_entry_zone = merge(allowed_left_entry_zone_structures) if allowed_left_entry_zone_structures else None\n",
    "if merged_allowed_left_entry_zone: merged_allowed_left_entry_zone.alpha(0.04)\n",
    "\n",
    "merged_allowed_right_entry_zone = merge(allowed_right_entry_zone_structures) if allowed_right_entry_zone_structures else None\n",
    "if merged_allowed_right_entry_zone: merged_allowed_right_entry_zone.alpha(0.04)\n",
    "\n",
    "merged_left_stn = merge(left_stn_structures) if left_stn_structures else None\n",
    "merged_right_stn = merge(right_stn_structures) if right_stn_structures else None\n",
    "\n",
    "merged_obstacles = merge(obstacle_structures) if obstacle_structures else None\n",
    "if merged_obstacles: merged_obstacles.c(\"red\").alpha(0.75)\n",
    "\n",
    "merged_full_brain = merge(full_brain_structures) if full_brain_structures else None\n",
    "if merged_full_brain: merged_full_brain.c(\"grey\").alpha(0.01)\n",
    "\n",
    "print(\"\\nMerged Summary:\")\n",
    "print(f\"  Left Entry Zone: {'yes' if merged_left_entry_zone else 'no'}\")\n",
    "print(f\"  Right Entry Zone: {'yes' if merged_right_entry_zone else 'no'}\")\n",
    "print(f\"  Left Allowed Entry Zone: {'yes' if merged_allowed_left_entry_zone else 'no'}\")\n",
    "print(f\"  Right Allowed Entry Zone: {'yes' if merged_allowed_right_entry_zone else 'no'}\")\n",
    "print(f\"  Left STN: {'yes' if merged_left_stn else 'no'}\")\n",
    "print(f\"  Right STN: {'yes' if merged_right_stn else 'no'}\")\n",
    "print(f\"  Obstacles: {'yes' if merged_obstacles else 'no'}\")\n",
    "print(f\"  Full Brain: {'yes' if merged_full_brain else 'no'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6236a915",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b2a262",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_path_length(start_point, end_point):\n",
    "    \"\"\"Compute the distance between two points for the line path.\"\"\"\n",
    "    return np.linalg.norm(np.array(end_point) - np.array(start_point))\n",
    "\n",
    "\n",
    "def compute_obstacle_distance(start_point, end_point):\n",
    "    \"\"\"Compute the minimum distance from the line path to any obstacle.\"\"\"\n",
    "    if merged_obstacles is None:\n",
    "        return 0.0\n",
    "    temp_line = Line([start_point, end_point])\n",
    "    d = temp_line.distance_to(merged_obstacles)\n",
    "    if isinstance(d, (list, tuple, np.ndarray)):\n",
    "        return float(np.min(d))\n",
    "    return float(d)\n",
    "\n",
    "\n",
    "def check_intersection(start_point, end_point):\n",
    "    \"\"\"Check if the line path intersects any obstacle.\"\"\"\n",
    "    if merged_obstacles is None:\n",
    "        return False\n",
    "    intersections = merged_obstacles.intersect_with_line(start_point, end_point)\n",
    "    return intersections is not None and len(intersections) > 0\n",
    "\n",
    "\n",
    "def _closest_point_on_mesh(point, mesh):\n",
    "    \"\"\"Project a 3D point onto the closest point on a mesh surface.\"\"\"\n",
    "    if mesh is None:\n",
    "        return tuple(point)\n",
    "    try:\n",
    "        cp = mesh.closest_point(point)\n",
    "        return tuple(cp)\n",
    "    except Exception:\n",
    "        # Fallback to nearest vertex\n",
    "        verts = np.asarray(mesh.vertices)\n",
    "        if verts.size == 0:\n",
    "            return tuple(point)\n",
    "        idx = int(np.argmin(np.linalg.norm(verts - np.asarray(point), axis=1)))\n",
    "        return tuple(verts[idx])\n",
    "\n",
    "\n",
    "def project_to_surfaces(start_point, end_point, hemisphere):\n",
    "    \"\"\"Project start/end onto entry zone and STN surfaces for the chosen hemisphere.\"\"\"\n",
    "    if hemisphere == 'left':\n",
    "        sp = _closest_point_on_mesh(start_point, merged_allowed_left_entry_zone)\n",
    "        ep = _closest_point_on_mesh(end_point, merged_left_stn)\n",
    "    else:\n",
    "        sp = _closest_point_on_mesh(start_point, merged_allowed_right_entry_zone)\n",
    "        ep = _closest_point_on_mesh(end_point, merged_right_stn)\n",
    "    return sp, ep\n",
    "\n",
    "\n",
    "def sample_start_and_end():\n",
    "    \"\"\"Sample a hemisphere-consistent start (entry zone) and end (STN).\"\"\"\n",
    "    hemispheres = []\n",
    "    if merged_allowed_left_entry_zone and merged_left_stn:\n",
    "        hemispheres.append('left')\n",
    "    if merged_allowed_right_entry_zone and merged_right_stn:\n",
    "        hemispheres.append('right')\n",
    "    if not hemispheres:\n",
    "        raise RuntimeError(\"No valid hemisphere data available for sampling.\")\n",
    "    hemi = random.choice(hemispheres)\n",
    "    if hemi == 'left':\n",
    "        start = tuple(merged_allowed_left_entry_zone.generate_random_points(1).points[0])\n",
    "        end = tuple(merged_left_stn.generate_random_points(1).points[0])\n",
    "    else:\n",
    "        start = tuple(merged_allowed_right_entry_zone.generate_random_points(1).points[0])\n",
    "        end = tuple(merged_right_stn.generate_random_points(1).points[0])\n",
    "    return hemi, start, end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0c6347",
   "metadata": {},
   "source": [
    "## 2. Monte Carlo Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca9c87d",
   "metadata": {},
   "source": [
    "### Path Generation and Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33158c07",
   "metadata": {},
   "source": [
    "Generate random paths from brain surface to STN and evaluate them:\n",
    "- **Orange lines**: Failed paths (intersect obstacles)\n",
    "- **Yellow lines**: Successful but suboptimal paths\n",
    "- **Green line**: Optimal path (maximum distance from obstacles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746bda9b",
   "metadata": {},
   "source": [
    "**Optimization Algorithm**\n",
    "\n",
    "Runs until finding a successful path and after a minimum of `min_attempts`.\n",
    "\n",
    "Chooses a **random location** from the viable brain surface region and the STN of the corresponding hemisphere. \n",
    "\n",
    "For each plot, the distance from obstacles is saved. The \"best\" path is determined using the highest distance to obstacles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef872cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for Monte Carlo path generation\n",
    "max_attempts = 1000\n",
    "min_attempts = 100\n",
    "successful_attempts = []  # Store (hemisphere, start_point, end_point, obstacle_distance, path_length) tuples\n",
    "failed_lines = []\n",
    "\n",
    "# Line width configuration (for visualization)\n",
    "BEST_PATH_WIDTH = 8\n",
    "SUBOPTIMAL_PATH_WIDTH = 5\n",
    "FAILED_PATH_WIDTH = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c3a370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and evaluate hemisphere-consistent paths\n",
    "for attempt in range(max_attempts):\n",
    "    if attempt + 1 > min_attempts:\n",
    "        break\n",
    "\n",
    "    hemi, start_point, end_point = sample_start_and_end()\n",
    "    print(f\"Attempt {attempt+1}: hemisphere={hemi} start={start_point} end={end_point}\")\n",
    "\n",
    "    # Check for intersection if we have obstacles\n",
    "    if merged_obstacles:\n",
    "        if not check_intersection(start_point, end_point):\n",
    "            # Compute metrics\n",
    "            temp_line = Line([start_point, end_point])\n",
    "            obstacle_dist = compute_obstacle_distance(start_point, end_point)\n",
    "            path_len = compute_path_length(start_point, end_point)\n",
    "            successful_attempts.append((hemi, start_point, end_point, obstacle_dist, path_len))\n",
    "            print(f\"  SUCCESS - Obstacle Distance: {obstacle_dist:.3f} Path Length: {path_len:.3f}\")\n",
    "        else:\n",
    "            failed_line = Line([start_point, end_point]).c(\"orange\").alpha(0.3).lw(FAILED_PATH_WIDTH)\n",
    "            failed_lines.append(failed_line)\n",
    "            print(\"  FAIL - Line intersects obstacles\")\n",
    "    else:\n",
    "        obstacle_dist = 0.0\n",
    "        path_len = compute_path_length(start_point, end_point)\n",
    "        successful_attempts.append((hemi, start_point, end_point, obstacle_dist, path_len))\n",
    "        print(f\"  SUCCESS (no obstacles) Path Length: {path_len:.3f}\")\n",
    "else:\n",
    "    print(f\"Warning: Only found {len(successful_attempts)} non-intersecting lines after {max_attempts} attempts\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee6a906",
   "metadata": {},
   "source": [
    "### Select Optimal Path\n",
    "\n",
    "Find the best path among successful attempts by maximizing distance from obstacles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b30bbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best path (maximum obstacle distance; tie-breaker shorter path length)\n",
    "best_path = None\n",
    "suboptimal_lines = []\n",
    "selected_hemisphere = None\n",
    "\n",
    "if successful_attempts:\n",
    "    # Sort: first by obstacle distance desc, then by path length asc\n",
    "    successful_attempts.sort(key=lambda x: (-x[3], x[4]))\n",
    "\n",
    "    best_hemi, best_start, best_end, best_obstacle_dist, best_path_len = successful_attempts[0]\n",
    "    best_path = Line([best_start, best_end]).c(\"green\").lw(BEST_PATH_WIDTH)\n",
    "    selected_hemisphere = best_hemi\n",
    "    print(f\"\\nBest path: hemisphere={best_hemi} obstacle_dist={best_obstacle_dist:.3f} path_len={best_path_len:.3f}\")\n",
    "\n",
    "    max_suboptimal_paths = 10\n",
    "    s = 1\n",
    "    for hemi, start_pt, end_pt, obstacle_dist, path_len in successful_attempts[1:]:\n",
    "        if s > max_suboptimal_paths:\n",
    "            break\n",
    "        \n",
    "        suboptimal_line = Line([start_pt, end_pt]).c(\"yellow\").alpha(0.3).lw(SUBOPTIMAL_PATH_WIDTH)\n",
    "        suboptimal_lines.append(suboptimal_line)\n",
    "        print(f\"Suboptimal path {s}: hemi={hemi} obstacle_dist={obstacle_dist:.3f} path_len={path_len:.3f}\")\n",
    "        \n",
    "        s += 1\n",
    "else:\n",
    "    print(\"No successful paths found!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d8b522",
   "metadata": {},
   "source": [
    "## 3. Weighted Sum + Nelder-Mead Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4eb6cd",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "Set weights and optimization parameters:\n",
    "- `w_obstacle`: Weight for obstacle distance (maximize)\n",
    "- `w_path_length`: Weight for path length (minimize)\n",
    "- Weights are normalized automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc3eb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights for multi-objective optimization (easily configurable)\n",
    "w_obstacle = 0.9     # Weight for obstacle distance (higher = prefer safer paths)\n",
    "w_path_length = 0.1   # Weight for path length (higher = prefer shorter paths)\n",
    "\n",
    "# Normalize weights to sum to 1\n",
    "total_weight = w_obstacle + w_path_length\n",
    "w_obstacle_norm = w_obstacle / total_weight\n",
    "w_path_length_norm = w_path_length / total_weight\n",
    "\n",
    "print(f\"Objective function weights:\\n  Obstacle distance: {w_obstacle_norm:.3f}\\n  Path length: {w_path_length_norm:.3f}\")\n",
    "\n",
    "# Nelder-Mead parameters\n",
    "initial_point_attempts = 50\n",
    "nm_max_iterations = 5000\n",
    "nm_tolerance = 1e-6  # default: 1e-6, applied as xatol and fatol\n",
    "\n",
    "# Hemisphere lock for optimization (set later when picking x0)\n",
    "nm_hemisphere = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195ac81b",
   "metadata": {},
   "source": [
    "### Weighted Sum Objective Function\n",
    "\n",
    "Combines both objectives (obstacle distance and path length) into a single scalar value for optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c69c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables for normalization (will be computed from sample paths)\n",
    "max_obstacle_dist = None\n",
    "max_path_length = None\n",
    "\n",
    "def weighted_sum_objective(x):\n",
    "    \"\"\"\n",
    "    Weighted sum objective function for optimization.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    x : array-like, shape (6,)\n",
    "        [start_x, start_y, start_z, end_x, end_y, end_z]\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    float : Objective value (to be minimized)\n",
    "    \"\"\"\n",
    "    global nm_hemisphere\n",
    "    start_guess = tuple(x[:3])\n",
    "    end_guess = tuple(x[3:])\n",
    "\n",
    "    # If hemisphere not set, infer a reasonable default\n",
    "    if nm_hemisphere is None:\n",
    "        nm_hemisphere = 'left' if merged_left_entry_zone and merged_left_stn else 'right'\n",
    "\n",
    "    # Project guesses to valid surfaces for the locked hemisphere\n",
    "    start_point, end_point = project_to_surfaces(start_guess, end_guess, nm_hemisphere)\n",
    "\n",
    "    # Hard constraint: penalize intersection\n",
    "    if check_intersection(start_point, end_point):\n",
    "        return 1e10\n",
    "\n",
    "    # Compute objectives\n",
    "    obstacle_dist = compute_obstacle_distance(start_point, end_point)\n",
    "    path_length = compute_path_length(start_point, end_point)\n",
    "\n",
    "    # Normalize objectives\n",
    "    if max_obstacle_dist is not None and max_obstacle_dist > 0:\n",
    "        norm_obstacle_dist = obstacle_dist / max_obstacle_dist\n",
    "    else:\n",
    "        norm_obstacle_dist = obstacle_dist / 100.0\n",
    "\n",
    "    if max_path_length is not None and max_path_length > 0:\n",
    "        norm_path_length = path_length / max_path_length\n",
    "    else:\n",
    "        norm_path_length = path_length / 200.0\n",
    "\n",
    "    # Weighted sum: minimize (-obstacle_dist) and minimize path_length\n",
    "    objective = -w_obstacle_norm * norm_obstacle_dist + w_path_length_norm * norm_path_length\n",
    "    return objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec1d2ba",
   "metadata": {},
   "source": [
    "### Estimate Normalization Constants\n",
    "\n",
    "Sample a few random paths to estimate typical ranges for obstacle distance and path length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9f5a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "ESTIMATION_SAMPLES = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6c45bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample random paths to estimate normalization constants\n",
    "print(\"Estimating normalization constants from random samples...\")\n",
    "sample_obstacle_dists = []\n",
    "sample_path_lengths = []\n",
    "\n",
    "for _ in range(ESTIMATION_SAMPLES):\n",
    "    print(f\"Sampling path {_+1}/{ESTIMATION_SAMPLES}...\")\n",
    "    hemi, start, end = sample_start_and_end()\n",
    "    if not check_intersection(start, end):\n",
    "        obstacle_dist = compute_obstacle_distance(start, end)\n",
    "        path_length = compute_path_length(start, end)\n",
    "        sample_obstacle_dists.append(obstacle_dist)\n",
    "        sample_path_lengths.append(path_length)\n",
    "        # print(f\"  obstacle_dist={obstacle_dist:.3f} path_len={path_length:.3f}\")\n",
    "    else:\n",
    "        print(\"  Sampled path intersects obstacles, skipping...\")\n",
    "\n",
    "if sample_obstacle_dists:\n",
    "    max_obstacle_dist = max(sample_obstacle_dists)\n",
    "    max_path_length = max(sample_path_lengths)\n",
    "    print(f\"Estimated max obstacle distance: {max_obstacle_dist:.2f}\")\n",
    "    print(f\"Estimated max path length: {max_path_length:.2f}\")\n",
    "else:\n",
    "    print(\"Warning: No valid paths found for normalization, using defaults\")\n",
    "    max_obstacle_dist = 8\n",
    "    max_path_length = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7329effd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## OPTION 2: Use Monte Carlo results for normalization (if available)\n",
    "if successful_attempts:\n",
    "    print(\"\\nUsing Monte Carlo results for normalization...\")\n",
    "    mc_obstacle_dists = [x[3] for x in successful_attempts]\n",
    "    mc_path_lengths = [x[4] for x in successful_attempts]\n",
    "    max_obstacle_dist = max(mc_obstacle_dists)\n",
    "    max_path_length = max(mc_path_lengths)\n",
    "    print(f\"Max obstacle distance from Monte Carlo: {max_obstacle_dist:.2f}\")\n",
    "    print(f\"Max path length from Monte Carlo: {max_path_length:.2f}\")\n",
    "else:\n",
    "    print(\"\\nNo Monte Carlo results available for normalization, using defaults\")\n",
    "    max_obstacle_dist = 8\n",
    "    max_path_length = 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c52aacb",
   "metadata": {},
   "source": [
    "### Set Initial Path for Optimization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3768a97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## OPTION 1: Find first feasible initial point from random sampling\n",
    "print(\"\\nFinding feasible initial point...\")\n",
    "x0 = None\n",
    "for attempt in range(initial_point_attempts):\n",
    "    hemi, start, end = sample_start_and_end()\n",
    "    if not check_intersection(start, end):\n",
    "        x0 = np.concatenate([start, end])\n",
    "        nm_hemisphere = hemi  # lock hemisphere for optimization\n",
    "        print(f\"Found feasible starting point after {attempt+1} attempts (hemi={hemi})\")\n",
    "        print(f\"  Start: {start}\")\n",
    "        print(f\"  End: {end}\")\n",
    "        break\n",
    "\n",
    "if x0 is None:\n",
    "    print(\"Warning: Could not find feasible starting point, using last sampled anyway\")\n",
    "    hemi, start, end = sample_start_and_end()\n",
    "    x0 = np.concatenate([start, end])\n",
    "    nm_hemisphere = hemi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24480db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## OPTION 2: Select best Monte Carlo path as starting point (if available)\n",
    "if successful_attempts:\n",
    "    best_hemi, best_start, best_end, best_obstacle_dist, best_path_len = successful_attempts[0]\n",
    "    print(\"\\nUsing best Monte Carlo path as initial point for optimization...\")\n",
    "    x0 = np.concatenate([best_start, best_end])\n",
    "    nm_hemisphere = best_hemi\n",
    "    selected_hemisphere = best_hemi\n",
    "else:\n",
    "    print(\"\\nNo Monte Carlo successful paths available; retaining previous x0 and hemisphere.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f55ed6",
   "metadata": {},
   "source": [
    "### Run Nelder-Mead Optimization\n",
    "\n",
    "Use Nelder-Mead simplex algorithm to optimize the weighted sum objective. Starts from a random feasible point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90600b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Nelder-Mead optimization\n",
    "print(f\"\\nRunning Nelder-Mead optimization (hemi={nm_hemisphere})...\")\n",
    "print(f\"  Max iterations: {nm_max_iterations}\")\n",
    "print(f\"  Tolerance: {nm_tolerance}\")\n",
    "\n",
    "result = minimize(\n",
    "    weighted_sum_objective,\n",
    "    x0,\n",
    "    method='Nelder-Mead',\n",
    "    options={\n",
    "        'maxiter': nm_max_iterations,\n",
    "        'xatol': nm_tolerance,\n",
    "        'fatol': nm_tolerance,\n",
    "        'disp': True\n",
    "    }\n",
    ")\n",
    "\n",
    "raw_start = tuple(result.x[:3])\n",
    "raw_end = tuple(result.x[3:])\n",
    "nm_optimal_start, nm_optimal_end = project_to_surfaces(raw_start, raw_end, nm_hemisphere)\n",
    "nm_optimal_objective = result.fun\n",
    "selected_hemisphere = nm_hemisphere\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Optimization Results:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Success: {result.success}\")\n",
    "print(f\"Message: {result.message}\")\n",
    "print(f\"Iterations: {result.nit}\")\n",
    "print(f\"Function evaluations: {result.nfev}\")\n",
    "print(f\"Final objective value: {nm_optimal_objective:.6f}\")\n",
    "print(f\"\\nOptimal path (projected, hemi={nm_hemisphere}):\")\n",
    "print(f\"  Start: {nm_optimal_start}\")\n",
    "print(f\"  End: {nm_optimal_end}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39ee7a7",
   "metadata": {},
   "source": [
    "### Evaluate Optimal Path\n",
    "\n",
    "Compute detailed metrics for the optimized path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bf0eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the optimal path\n",
    "nm_obstacle_dist = compute_obstacle_distance(nm_optimal_start, nm_optimal_end)\n",
    "nm_path_length = compute_path_length(nm_optimal_start, nm_optimal_end)\n",
    "nm_has_intersection = check_intersection(nm_optimal_start, nm_optimal_end)\n",
    "\n",
    "print(f\"\\nOptimal Path Metrics:\")\n",
    "print(f\"  Obstacle distance: {nm_obstacle_dist:.3f}\")\n",
    "print(f\"  Path length: {nm_path_length:.3f}\")\n",
    "print(f\"  Intersects obstacles: {nm_has_intersection}\")\n",
    "\n",
    "# Create line for visualization\n",
    "nm_optimal_line = Line([nm_optimal_start, nm_optimal_end]).c(\"cyan\").lw(10)\n",
    "print(f\"\\nOptimal path created (cyan line)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83102dc",
   "metadata": {},
   "source": [
    "### Compare with Monte Carlo Results\n",
    "\n",
    "Compare the Nelder-Mead optimized path with the best Monte Carlo path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93959517",
   "metadata": {},
   "outputs": [],
   "source": [
    "if successful_attempts and nm_optimal_line is not None:\n",
    "    mc_best_hemi, mc_best_start, mc_best_end, mc_best_obstacle_dist, mc_best_path_len = successful_attempts[0]\n",
    "    mc_best_path_length = compute_path_length(mc_best_start, mc_best_end)\n",
    "    \n",
    "    # Compute weighted sum for Monte Carlo best\n",
    "    norm_mc_obstacle = mc_best_obstacle_dist / max_obstacle_dist if max_obstacle_dist > 0 else 0\n",
    "    norm_mc_length = mc_best_path_length / max_path_length if max_path_length > 0 else 0\n",
    "    mc_weighted_sum = -w_obstacle_norm * norm_mc_obstacle + w_path_length_norm * norm_mc_length\n",
    "    \n",
    "    # Compute weighted sum for Nelder-Mead\n",
    "    norm_nm_obstacle = nm_obstacle_dist / max_obstacle_dist if max_obstacle_dist > 0 else 0\n",
    "    norm_nm_length = nm_path_length / max_path_length if max_path_length > 0 else 0\n",
    "    nm_weighted_sum = -w_obstacle_norm * norm_nm_obstacle + w_path_length_norm * norm_nm_length\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Comparison: Monte Carlo vs Weighted Sum Optimization\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"\\nMonte Carlo (best of {len(successful_attempts)} successful):\")\n",
    "    print(f\"  Obstacle distance: {mc_best_obstacle_dist:.3f}\")\n",
    "    print(f\"  Path length: {mc_best_path_length:.3f}\")\n",
    "    print(f\"  Weighted sum: {mc_weighted_sum:.6f}\")\n",
    "    \n",
    "    print(f\"\\nWeighted Sum Optimization:\")\n",
    "    print(f\"  Obstacle distance: {nm_obstacle_dist:.3f}\")\n",
    "    print(f\"  Path length: {nm_path_length:.3f}\")\n",
    "    print(f\"  Weighted sum: {nm_weighted_sum:.6f}\")\n",
    "    \n",
    "    print(f\"\\nImprovement:\")\n",
    "    obstacle_improvement = ((nm_obstacle_dist - mc_best_obstacle_dist) / mc_best_obstacle_dist * 100) if mc_best_obstacle_dist > 0 else 0\n",
    "    length_improvement = ((mc_best_path_length - nm_path_length) / mc_best_path_length * 100) if mc_best_path_length > 0 else 0\n",
    "    weighted_improvement = ((mc_weighted_sum - nm_weighted_sum) / abs(mc_weighted_sum) * 100) if mc_weighted_sum != 0 else 0\n",
    "    \n",
    "    print(f\"  Obstacle distance: {obstacle_improvement:+.2f}%\")\n",
    "    print(f\"  Path length: {length_improvement:+.2f}% (shorter is better)\")\n",
    "    print(f\"  Weighted sum objective: {weighted_improvement:+.2f}% (lower is better)\")\n",
    "else:\n",
    "    print(\"No Monte Carlo results available for comparison\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68254908",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34818bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "actors = []\n",
    "if merged_full_brain: actors.append(merged_full_brain)\n",
    "\n",
    "if selected_hemisphere == 'left':\n",
    "    if merged_left_entry_zone:\n",
    "        actors.append(merged_left_entry_zone.clone().alpha(0.08))\n",
    "    if merged_right_entry_zone:\n",
    "        actors.append(merged_right_entry_zone.clone().alpha(0.02))\n",
    "    if merged_left_stn:\n",
    "        actors.append(merged_left_stn.clone().alpha(0.9))\n",
    "    if merged_right_stn:\n",
    "        actors.append(merged_right_stn.clone().alpha(0.3))\n",
    "elif selected_hemisphere == 'right':\n",
    "    if merged_left_entry_zone:\n",
    "        actors.append(merged_left_entry_zone.clone().alpha(0.02))\n",
    "    if merged_right_entry_zone:\n",
    "        actors.append(merged_right_entry_zone.clone().alpha(0.08))\n",
    "    if merged_left_stn:\n",
    "        actors.append(merged_left_stn.clone().alpha(0.3))\n",
    "    if merged_right_stn:\n",
    "        actors.append(merged_right_stn.clone().alpha(0.9))\n",
    "else:\n",
    "    if merged_left_entry_zone: actors.append(merged_left_entry_zone)\n",
    "    if merged_right_entry_zone: actors.append(merged_right_entry_zone)\n",
    "    if merged_left_stn: actors.append(merged_left_stn)\n",
    "    if merged_right_stn: actors.append(merged_right_stn)\n",
    "\n",
    "if merged_obstacles: actors.append(merged_obstacles)\n",
    "\n",
    "actors.extend(failed_lines)\n",
    "actors.extend(suboptimal_lines)\n",
    "if best_path:\n",
    "    mc_best_faded = best_path.clone().alpha(0.3)\n",
    "    actors.append(mc_best_faded)\n",
    "\n",
    "try:\n",
    "    actors.append(nm_optimal_line)\n",
    "except:\n",
    "    print(\"Nelder-Mead optimized path not found.\")\n",
    "\n",
    "print(f\"Rendering scene with optimized path (cyan)...\")\n",
    "show(actors, axes=1, viewup=\"z\", \n",
    "     title=f\"Optimized Path\", new=True, interactive=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7069ffe2",
   "metadata": {},
   "source": [
    "#### K3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9e5148",
   "metadata": {},
   "outputs": [],
   "source": [
    "import k3d\n",
    "\n",
    "# Helper function to convert vedo mesh to k3d mesh\n",
    "def vedo_to_k3d_mesh(vedo_mesh, color=None, opacity_factor=1.0):\n",
    "    vertices = np.array(vedo_mesh.vertices, dtype=np.float32)\n",
    "    faces = np.array(vedo_mesh.cells, dtype=np.uint32)\n",
    "    \n",
    "    if color is None:\n",
    "        vedo_color = vedo_mesh.color()\n",
    "        if isinstance(vedo_color, str):\n",
    "            import matplotlib.colors as mcolors\n",
    "            if vedo_color in mcolors.CSS4_COLORS:\n",
    "                color_hex = mcolors.CSS4_COLORS[vedo_color]\n",
    "                color = int(color_hex.lstrip('#'), 16)\n",
    "            else:\n",
    "                color = 0x808080\n",
    "        elif isinstance(vedo_color, (list, tuple, np.ndarray)):\n",
    "            # Handle RGB tuples/arrays (values 0-1 or 0-255)\n",
    "            rgb = np.asarray(vedo_color[:3])\n",
    "            # Normalize to 0-255 range if needed\n",
    "            if rgb.max() <= 1.0:\n",
    "                rgb = (rgb * 255).astype(int)\n",
    "            else:\n",
    "                rgb = rgb.astype(int)\n",
    "            r, g, b = int(rgb[0]), int(rgb[1]), int(rgb[2])\n",
    "            color = (r << 16) | (g << 8) | b\n",
    "        else:\n",
    "            color = 0x808080\n",
    "    \n",
    "    vedo_alpha = vedo_mesh.alpha()\n",
    "    if vedo_alpha is not None:\n",
    "        opacity = float(vedo_alpha)\n",
    "    else:\n",
    "        opacity = 1.0\n",
    "        \n",
    "    opacity = np.clip(opacity * opacity_factor, 0.0, 1.0)\n",
    "    \n",
    "    return k3d.mesh(vertices, faces, color=color, opacity=opacity)\n",
    "\n",
    "def vedo_line_to_k3d(vedo_line, color=None, width=0.1):\n",
    "    line_scale_factor = 0.2\n",
    "    points = np.array(vedo_line.vertices, dtype=np.float32)\n",
    "    \n",
    "    if color is None:\n",
    "        vedo_color = vedo_line.color()\n",
    "        if isinstance(vedo_color, str):\n",
    "            import matplotlib.colors as mcolors\n",
    "            if vedo_color in mcolors.CSS4_COLORS:\n",
    "                color_hex = mcolors.CSS4_COLORS[vedo_color]\n",
    "                color = int(color_hex.lstrip('#'), 16)\n",
    "            else:\n",
    "                color = 0x00FF00\n",
    "        elif isinstance(vedo_color, (list, tuple, np.ndarray)):\n",
    "            # Handle RGB tuples/arrays (values 0-1 or 0-255)\n",
    "            rgb = np.asarray(vedo_color[:3])\n",
    "            # Normalize to 0-255 range if needed\n",
    "            if rgb.max() <= 1.0:\n",
    "                rgb = (rgb * 255).astype(int)\n",
    "            else:\n",
    "                rgb = rgb.astype(int)\n",
    "            r, g, b = int(rgb[0]), int(rgb[1]), int(rgb[2])\n",
    "            color = (r << 16) | (g << 8) | b\n",
    "        else:\n",
    "            color = 0x00FF00\n",
    "    \n",
    "    return k3d.line(points, color=color, width=width * line_scale_factor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cb95b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot optimization results with k3d\n",
    "plot = k3d.plot(name=f'Optimized Path (w_obs={w_obstacle_norm:.2f}, w_len={w_path_length_norm:.2f})')\n",
    "\n",
    "if merged_full_brain:\n",
    "    plot += vedo_to_k3d_mesh(merged_full_brain)\n",
    "\n",
    "if selected_hemisphere == 'left':\n",
    "    print(\"Left hemisphere focused.\")\n",
    "    if merged_left_entry_zone:\n",
    "        plot += vedo_to_k3d_mesh(merged_left_entry_zone, opacity_factor=2)\n",
    "    if merged_right_entry_zone:\n",
    "        plot += vedo_to_k3d_mesh(merged_right_entry_zone, opacity_factor=0.5)\n",
    "    if merged_left_stn:\n",
    "        plot += vedo_to_k3d_mesh(merged_left_stn, opacity_factor=1)\n",
    "    if merged_right_stn:\n",
    "        plot += vedo_to_k3d_mesh(merged_right_stn, opacity_factor=0.5)\n",
    "elif selected_hemisphere == 'right':\n",
    "    print(\"Right hemisphere focused.\")\n",
    "    if merged_left_entry_zone:\n",
    "        plot += vedo_to_k3d_mesh(merged_left_entry_zone, opacity_factor=0.5)\n",
    "    if merged_right_entry_zone:\n",
    "        plot += vedo_to_k3d_mesh(merged_right_entry_zone, opacity_factor=2)\n",
    "    if merged_left_stn:\n",
    "        plot += vedo_to_k3d_mesh(merged_left_stn, opacity_factor=0.5)\n",
    "    if merged_right_stn:\n",
    "        plot += vedo_to_k3d_mesh(merged_right_stn, opacity_factor=1)\n",
    "else:\n",
    "    if merged_left_entry_zone:\n",
    "        plot += vedo_to_k3d_mesh(merged_left_entry_zone)\n",
    "    if merged_right_entry_zone:\n",
    "        plot += vedo_to_k3d_mesh(merged_right_entry_zone)\n",
    "    if merged_left_stn:\n",
    "        plot += vedo_to_k3d_mesh(merged_left_stn)\n",
    "    if merged_right_stn:\n",
    "        plot += vedo_to_k3d_mesh(merged_right_stn)\n",
    "\n",
    "if merged_obstacles:\n",
    "    plot += vedo_to_k3d_mesh(merged_obstacles)\n",
    "\n",
    "for line in failed_lines:\n",
    "    plot += vedo_line_to_k3d(line, color=0xFF5F1F, width=FAILED_PATH_WIDTH * 0.5)\n",
    "for line in suboptimal_lines:\n",
    "    plot += vedo_line_to_k3d(line, color=0xFFFF00, width=SUBOPTIMAL_PATH_WIDTH * 0.5)\n",
    "if best_path:\n",
    "    plot += vedo_line_to_k3d(best_path, color=0x00FF00, width=BEST_PATH_WIDTH * 0.7)\n",
    "\n",
    "try:\n",
    "    plot += vedo_line_to_k3d(nm_optimal_line, color=0x00FFFF, width=BEST_PATH_WIDTH * 1.0)\n",
    "except: \n",
    "    print(\"Nelder-Mead optimized path not found.\")\n",
    "\n",
    "print(\"Displaying optimization result with k3d...\")\n",
    "plot.display()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
